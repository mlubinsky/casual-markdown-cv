<!DOCTYPE html>
<title>Resume</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/casualwriter/casual-markdown/dist/casual-markdown.css">
<script src="https://cdn.jsdelivr.net/gh/casualwriter/casual-markdown/dist/casual-markdown.js"></script>
<style>  
  body { line-height:1.5; margin:auto; padding:3px; max-width:1024px; display:none; FONT-FAMILY:"Segoe UI",ARIAL; }
  h1  { font-size:200%; padding:16px; border:1px solid lightgrey; BACKGROUND:#f0f0f0; }
  h2  { border-bottom:1px solid grey; padding:2px }
</style>
<body onload="document.body.innerHTML=md.html(document.body.innerHTML); document.body.style.display='block';">
<!--======= COPY ABOVE CODE AS HEADER, THEN FOLLOW WITH RESUME CONTENT IN MARKDOWN FORMAT =========-->

<span style="float:right;padding:6px"> 
  mlubinsky@hotmail.com <br> mobile: 408 775 4518 <br> Nationality: USA
</span>

# Michael Lubinsky  

## Executive Summary

* More then 10 years experience building data pipelines and doing data analysis
* Big Data: Apache Spark, Apache Flink, Hadoop, Snowflake, Ni-Fi, Kafka  
* Databases: Postgres, MySQL, MariaDB, Oracle, Redshift, Presto, Hana, DynamoDB
* Languages: Python, Java, Scala, C, C++
* Apache Airflow, dbt
* Cloud platfoms: AWS, GCP, Databricks
* Data anaysis and machine learning: Pandas, sklearn, PyTorch, Tensorflow
* Reporting tools: Looker, Tableau, Superset

## Working Experience

#### Roku, San Jose, CA	  Apr 2020 – Present. Senior Data Engineer
•	Developing the data pipelines using Apache Airflow, Apache Spark, Redshift, Snowflake and Kafka
• Data acquisition, database design and implementation, dashboard/reporting using Looker and Tableau
• AWS  
•	Implemented time series data analysis and  anomaly detection algorithms
  
#### ARM, San Jose, CA	Apr 2018 – 2020. Senior Software Engineer

• Developing the data pipelines for IoT (Internet of Things): Data acquisition using MQTT protocol, database design and implementation
• Implemented time series data analysis, anomaly detection.
• Environment and tools:  AWS S3, Hadoop, Hive, Python, Java, SQL.

#### Apple, Cupertino, CA	Nov 2013 – Mar 2018.  Senior Software Engineer
• Developed the high performance, scalable and distributed pipeline: data acquisition, parsing, uploading, analysis and reporting.
• Implemented data clustering, supervised and non-supervised learning, time-series analysis.
• Developed database models and REST API, which is used by external applications.
• Environment and tools: Java, C++, Python, Oracle and no-SQL databases, RabbitMQ, Hadoop, Tableau.

#### SAP, Palo Alto, CA 	Nov 2011 – Oct 2013. Senior Software Engineer
• Created the logical and physical data modeling for HANA database; data partitioning and replication.
• Database performance profiling and tuning: # of threads, memory allocation.
• Run TPC-H database benchmarking
• Environment and tools: Linux, C++; Java, Python and SQL.

#### Yahoo Inc., Sunnyvale, CA 	Nov 2010 – Nov 2011. Senior Software Consultant in User Data and Analytics Group

I wrote map-reduce jobs for processing the hundreds of GB of data on Hadoop using Java and Pig Latin.
I implemented the sampling algorithm, which allows allocating the number of Hadoop reducers.
per key proportionally to the data distribution: as a result, the optimal performance is achieved. 
Applied statistical methods (regression, classification) for Apache log data analysis. 
Environment and tools: Linux, Java, Hadoop.

## Education & Qualifications

• Apache Spark, Udemy 2015
• Introduction to Artificial Intelligence, Coursera 2011
• Machine Learning, Coursera 2011
• MS in EE (with honor) - Moscow Institute of Electronic Engineering

 
